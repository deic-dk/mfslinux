#!/bin/bash

chroot /mnt

# Swapoff
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
# The above has no effect. We hack for now...
# This also has no effect with systemd
echo '@reboot sudo swapoff -a' >> /etc/crontab
# This has
systemctl stop `systemctl list-unit-files | grep \.swap | awk '{print $1}'`
systemctl mask `systemctl list-unit-files | grep \.swap | awk '{print $1}'`

# Add the google cloud package and opensuse repositories
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list
. /etc/os-release
sh -c "echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/x${NAME}_${VERSION_ID}/ /' > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list"
wget -nv https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/x${NAME}_${VERSION_ID}/Release.key -O- | apt-key add -

echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.19/xUbuntu_20.04/ /' | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:1.19.list
curl -fsSL https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:1.19/xUbuntu_20.04/Release.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/devel_kubic_libcontainers_stable_cri-o_1.19.gpg > /dev/null

# Install components
apt update
apt install -y kubelet kubeadm kubectl cri-o cri-o-runc cri-tools skopeo

# This bug is no longer present
#sed -i 's|/usr/lib/cri-o-runc/sbin/runc|/usr/sbin/runc|' /etc/crio/crio.conf.d/01-crio-runc.conf

# If we want to hold the version
#apt-mark hold kubelet kubeadm kubectl cri-o cri-o-runc

# Fix Ubuntu issues - apparently no longer needed
#sed -i 's|/var/run/dbus/system_bus_socket|/run/dbus/system_bus_socket|' /lib/systemd/system/dbus.socket
#chmod go+r /lib/systemd/system/kubelet.service
#echo "DefaultCPUAccounting=yes" >> /etc/systemd/system.conf

# Networking
modprobe overlay
modprobe br_netfilter
cat >> /etc/sysctl.d/99-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# IP forwarding
echo '1' > /proc/sys/net/ipv4/ip_forward
# Pass bridged packets through iptables
echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables
echo '1' > /proc/sys/net/bridge/bridge-nf-call-ip6tables
# Make permanent
sed -i 's|^#net.ipv4.ip_forward=1|net.ipv4.ip_forward=1|' /etc/sysctl.conf
echo 'net.bridge.bridge-nf-call-iptables=1' >> /etc/sysctl.conf
echo 'net.bridge.bridge-nf-call-ip6tables=1' >> /etc/sysctl.conf
# Have crio use 10.2.0.0 for pods instead of the default 10.85.0.0.
sed -i 's|10.85.0.0/16|10.2.0.0/16|' /etc/cni/net.d/100-crio-bridge.conf

sysctl -p
sudo sysctl --system

# Ubuntu 20.40 fix - see https://bugs.launchpad.net/ubuntu/+source/ifupdown/+bug/1874515
sed -i -E "s|^(# Finally we destroy the interface)|  brctl addbr br_nat_ext\n\n\1|" /lib/bridge-utils/ifupdown.sh

systemctl enable crio
systemctl start crio.service

# Necessary for kubctl to use port 6443 instead of 8080...
echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' >> /root/.bashrc

# Apparently /etc/default/kubelet and/or KUBELET_EXTRA_ARGS is not used in this version
#cat <<EOF | tee /etc/default/kubelet
#KUBELET_EXTRA_ARGS=--feature-gates="AllAlpha=false,RunAsGroup=true" \
#--container-runtime=remote \
#--cgroup-driver=systemd \
#--container-runtime-endpoint='unix:///var/run/crio/crio.sock' \
#--runtime-request-timeout=5m
#EOF

# Hardcode the service definition instead
#sed -i 's|/usr/bin/kubelet|/usr/bin/kubelet --feature-gates="AllAlpha=false,RunAsGroup=true" \\\
#	--container-runtime=remote \\\
#	--cgroup-driver=systemd \\\
#	--container-runtime-endpoint=unix:///var/run/crio/crio.sock \\\
#	--runtime-request-timeout=1m|' /lib/systemd/system/kubelet.service

# This does not work because it creates an /etc/kubernetes/kubelet.conf w/o "cgroupDriver: systemd"
# But it creates manifests etc.

# So... add the above line manually with kubeadm running
#kubeadm reset -f --cri-socket unix:///var/run/crio/crio.sock
kubeadm init --pod-network-cidr=10.2.0.0/16 --control-plane-endpoint=kube.sciencedata.dk \
--cri-socket=unix:///var/run/crio/crio.sock # --apiserver-advertise-address=10.0.0.12

# and do this in another shell...
echo "cgroupDriver: systemd" >> /var/lib/kubelet/config.yaml
sed -i -E 's|/usr/bin/kubelet$|/usr/bin/kubelet --cgroup-driver=systemd \
--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice \
--experimental-allowed-unsafe-sysctls=net.ipv4.ping_group_range|' \
/lib/systemd/system/kubelet.service

# This is written to /etc/resolv.conf inside the containers, so to allow pods/containers to
# use DNS this seems necessary. No idea why it is set to a service-cluster-ip-range address
dns_ips=`kubectl describe ep kube-dns --namespace=kube-system | grep -E '^ *Addresses:' | awk '{print $NF}' | sed "s|,|\\\\\n- |g" | sed 's|^|- |g'`
sed -i "s|- 10.96.0.10|${dns_ips}|" /var/lib/kubelet/config.yaml

# TODO: look into doing the above in a cleaner way - with a yaml file instead of command-line parameters, i.e.
# kubeadm init --config=kubeadm-config.yaml

# We don't need ipv6
sed -i.bk '/.*1100:200::\/24.*/d' /etc/cni/net.d/100-crio-bridge.conf

# Address Kubernetes bug
# https://cloudblue.freshdesk.com/support/solutions/articles/44001886522-how-to-fix-failed-to-get-system-container-stats-on-k8s-service-node-
cat <<EOF>/etc/systemd/system/kubelet.service.d/11-cgroups.conf
[Service]
CPUAccounting=true
MemoryAccounting=true
EOF

systemctl daemon-reload
systemctl restart kubelet

### HTTP networking with ScienceData
cat <<EOF>/etc/systemd/system/sciencedata_bridge.service
[Unit]
After=network.service
[Service]
ExecStart=/usr/local/sbin/sciencedata_bridge.sh
[Install]
WantedBy=default.target
EOF
cat <<EOF>/usr/local/sbin/sciencedata_bridge.sh
ip link add link enp129s0f0 name enp129s0f0.300 type vlan id 300
ip addr add 10.3.0.1/24 brd 10.3.0.255 dev enp129s0f0.300
ip link set dev enp129s0f0.300 up
ip addr add 10.2.0.254/16 dev enp129s0f0.300
brctl addif cni0 enp129s0f0.300
EOF
chmod +x /usr/local/sbin/sciencedata_bridge.sh
systemctl daemon-reload
systemctl enable sciencedata_bridge.service

### Install dashboard

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml

cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-public
EOF

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-public
EOF

# Forward port via ssh from a terminal on the host you're running your browser on
# ssh -L 8001:localhost:8001 root@kube.sciencedata.dk

# Run this on another terminal on kube.sciencedata.dk
# kubectl proxy --address='127.0.0.1' --accept-hosts='^.*$'

# Get token with
kubectl -n kube-public describe secret `kubectl -n kube-public get secret | grep admin-user | awk '{print $1}'`

# Paste it into the dashboard web interface on
# http://127.0.0.1:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

### NFS networking with ScienceData
git clone https://github.com/deic-dk/sciencedata_kubernetes.git
# Don't think this is necessary
#kubectl apply -f sciencedata_kubernetes/nfs_serviceaccount.yaml
#kubectl create rolebinding nfs-admin --clusterrole=admin --serviceaccount=default:nfs-client-provisioner --namespace=default
#kubectl create clusterrolebinding nfs-cluster-admin --clusterrole=cluster-admin --serviceaccount=default:nfs-client-provisioner

chmod +x /root/sciencedata_kubernetes/commands/*
cd /usr/local/bin
ln -s /root/sciencedata_kubernetes/commands/delete_pod 
ln -s /root/sciencedata_kubernetes/commands/get_containers 
ln -s /root/sciencedata_kubernetes/commands/run_pod 
